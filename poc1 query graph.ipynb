{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb69cb29-9782-47a2-8e2b-8495f6a9a685",
   "metadata": {},
   "source": [
    "# Application of LLM-Augmented Knowledge Graphs for Wirearchy Management\n",
    "\n",
    "### Universitat Oberta de Catalunya\n",
    "### Data Science Master's Degree - Data Analysis and Big Data.<br>Final project\n",
    "\n",
    "- Author: Xavier Ventura de los Ojos\n",
    "- Project Supervisor: Francesc Julbe López\n",
    "- Coordinating Professor: Albert Solé Ribalta\n",
    "- Date of submission: 06/2024\n",
    "\n",
    "## POC 1: How to query a KG using Natural Language\n",
    "\n",
    "In this POC we showcase how prompt engineering techniques using state of the art Large Language Models (LLM) can provide a Knowledge Graph with Natural Language query capabilities.\n",
    "\n",
    "It is not in the scope of the POC to build an application with chat or Q&A capabilities but to use the necessary building blocks and to measure the performance of these tools on a set of predefined questions.\n",
    "\n",
    "Instead of an interactive UI, the code in this Notebook allows performing tests on questions datasets (questions.json) using the indicated configuration (configurations.json). Questions, answers and related metadata is store in JSON files (poc1_answers folder) for proper analysis.\n",
    "\n",
    "For a detailed analysis of the interactions with the LLMs, it is recommended to configure [LangSmith](https://smith.langchain.com).\n",
    "\n",
    "These are the LLMs in scope of the POC:\n",
    "\n",
    "\n",
    "|Vendor| Model | Description |\n",
    "|---|---|--|\n",
    "|[OpenAI](https://openai.com)|[GPT-3.5 Turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)|The latest GPT-3.5 Turbo model with higher accuracy at responding in requested formats and a fix for a bug which caused a text encoding issue for non-English language function calls. Returns a maximum of 4,096 output tokens.|\n",
    "|[OpenAI](https://openai.com)|[GPT-4o](https://platform.openai.com/docs/models/gpt-4o)|Our most advanced, multimodal flagship model that’s cheaper and faster than GPT-4 Turbo. Currently points to gpt-4o-2024-05-13.|\n",
    "|[Anthropic](https://www.anthropic.com)|[Claude 3 Haiku](https://docs.anthropic.com/en/docs/models-overview)|Our most powerful model, delivering state-of-the-art performance on highly complex tasks and demonstrating fluency and human-like understanding|\n",
    "|[Anthropic](https://www.anthropic.com)|[Claude 3 Opus](https://docs.anthropic.com/en/docs/models-overview)|Our fastest and most compact model, designed for near-instant responsiveness and seamless AI experiences that mimic human interactions|\n",
    "\n",
    "> NOTE: Additional models can be easily incorporated to the test by adding the corresponding Chat in the *LLMS* dict and entry in the *configuration.json*. More details in the corresponding sections below.\n",
    "\n",
    "\n",
    "## Prerequisites and requirements\n",
    "\n",
    "The queries are executed on a Neo4j Graph database that needs to be created and fed as a prerequisite for the POCs 1 and 2.\n",
    "Detailed instructions are available as part of the thesis project documentation. \n",
    "\n",
    "This is the list of used python components and modules:\n",
    "\n",
    "| Component | Description |\n",
    "| ----- | ---- |\n",
    "| [LangChain](https://python.langchain.com/v0.1/docs/get_started/introduction) | Framework for developing applications powered by LLMs |\n",
    "| [GraphCypherQAChain](https://python.langchain.com/docs/integrations/graphs/neo4j_cypher) | Chain to interact with Neo4j graph database |\n",
    "| [OpenAI API Keys (login required)](https://platform.openai.com/api-keys) | API Keys to access OpenAI GPT3.5 and GPT4 models|\n",
    "| [Anthropic API Keys](https://docs.anthropic.com/en/api/getting-started) | API Keys to access Anthropic Haiku and Opus Models| "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe615960-3dd7-445e-a53b-637464ee9de8",
   "metadata": {},
   "source": [
    "## Initial setup \n",
    "\n",
    "The following API keys and Neo4j pwd are expected to be available as environment variables:\n",
    "\n",
    "- OPENAI_API_KEY: OpenAI API KEY.\n",
    "- ANTHROPIC_API_KEY: Anthropic API KEY.\n",
    "- LANGCHAIN_API_KEY: (Recommended).\n",
    "- NEO4J_PWD: Password of the Neo4j user.\n",
    "\n",
    "\n",
    "Execute the following %pip commands to install required packages if needed."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9a42b06-4fbe-4bb5-9399-3e946b3cee6c",
   "metadata": {},
   "source": [
    "%pip install neo4j\n",
    "%pip install jsonpickle\n",
    "%pip install -qU langchain-anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67116bbe-56fd-4140-9ac2-f00848fa33c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain.chains.graph_qa.prompts import CYPHER_GENERATION_PROMPT, CYPHER_QA_PROMPT\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.callbacks.manager import get_openai_callback\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "import neo4j\n",
    "import os, glob\n",
    "import json, pickle, jsonpickle\n",
    "import datetime, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c58585eb-ef45-4311-b628-a7e48cd7c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "# Project name in LangSmith\n",
    "LANGCHAIN_PROJECT = \"TFM\"\n",
    "\n",
    "# Folders\n",
    "ANSWERS_FOLDER = \"poc1_answers\"\n",
    "CONFIGS_FOLDER = \"poc1_config\"\n",
    "\n",
    "# Neo4j\n",
    "NEO4J_URL = \"bolt://localhost:7687\"\n",
    "NEO4J_USERNAME = \"neo4j\" \n",
    "\n",
    "# Text format\n",
    "TEXT_BOLD  = '\\033[1m'\n",
    "TEXT_BLUE  = '\\033[94m'\n",
    "TEXT_GREEN = '\\033[92m'\n",
    "TEXT_END   = '\\033[0m' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1db7dc1-2ab5-4416-ba1e-a64379150a5a",
   "metadata": {},
   "source": [
    "## Load the test configuration\n",
    "\n",
    "The test consists on the following components:\n",
    "\n",
    "- Dataset with 20 predefined **questions** related to the graph.\n",
    "- 2 **prompt templates** to guide the LLMs in order to improve its performance.\n",
    "- 8 different **scenarios** which are the choosen combinations of the 4 models and the 3 templates (1 default + 2 custom).\n",
    "\n",
    "The **questions** dataset is loaded from the *poc1_config/questions.json* file with the following structure:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        \"id\": \"Q001\",\n",
    "        \"question\": \"Qui és el president de la Generalitat de Catalunya?\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"Q002\",\n",
    "        \"question\": \"Qui té actualment el càrrec de 'President de la Generalitat de Catalunya'?\"\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "The different execution **scenarios** are loaded from the *poc1_config/configuration.json* file with the following structure:\n",
    "\n",
    "```\n",
    "{\n",
    "\"GPT35_P0\":  {\n",
    "        \"id\": \"GPT35_P0\",\n",
    "        \"description\": \"gpt-3.5-turbo default prompt\",\n",
    "        \"llm\": \"gpt-3.5-turbo\"\n",
    "    },\n",
    "\"GPT4O_P0\": {\n",
    "        \"id\": \"GPT4O_P0\",\n",
    "        \"description\": \"gpt-4o default prompt\",\n",
    "        \"llm\": \"gpt-4o\"\n",
    "    },\n",
    "    ...\n",
    "\"OPUS_P1\":  {\n",
    "        \"id\": \"OPUS_P1\",\n",
    "        \"description\": \"claude-3-opus complex prompt 1\",\n",
    "        \"llm\": \"claude-3-opus\",\n",
    "        \"cypher_prompt_template\": \"prompt_template_1\",\n",
    "        \"requests_per_minute\": 2\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "The different **prompt templates** are loaded from the *poc1_config/prompt_template_\\*.txt* files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4363f2cd-c46e-44ce-b465-a4f71e479fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper json functions \n",
    "\n",
    "def save_json(filename, data):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def load_json(path, filename): \n",
    "    file = os.path.join(path,filename)\n",
    "    with open(file,'r') as fp:\n",
    "        data = json.load(fp)\n",
    "    return data\n",
    "\n",
    "def save_jsonpickle(filename, data):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(jsonpickle.encode(data, unpicklable= False, indent=4))\n",
    "\n",
    "# Templates are stored in txt files for convenience\n",
    "def load_templates():\n",
    "    \"\"\"Load the templates from the TXT files located in the CONFIGS_FOLDER.\n",
    "    \n",
    "    Returns:\n",
    "        A dict with the templates. The key is the filename after removing the \".txt\" extension.\n",
    "    \"\"\"\n",
    "    templates={}\n",
    "    for filename in glob.glob(os.path.join(CONFIGS_FOLDER,\"prompt_template*.txt\")):\n",
    "        with open(filename, 'r') as file:\n",
    "            templates[filename.split(os.sep)[-1][:-4]] = file.read()\n",
    "    return templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7ef05a3-2058-4c81-bbb5-684b857ad328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the configurations\n",
    "\n",
    "TEMPLATES = load_templates()\n",
    "CONFIGURATIONS = load_json(CONFIGS_FOLDER,\"configurations.json\")\n",
    "QUESTIONS = load_json(CONFIGS_FOLDER,\"questions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c805f6f-62e7-4736-a2dc-7d7f4a222821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurations:\n",
      "\n",
      "\u001b[1mGPT35_P0\u001b[0m gpt-3.5-turbo default prompt \u001b[94m\u001b[0m\n",
      "\u001b[1mGPT4O_P0\u001b[0m gpt-4o default prompt \u001b[94m\u001b[0m\n",
      "\u001b[1mGPT35_P1\u001b[0m gpt-3.5-turbo complex prompt 1 \u001b[94mprompt_template_1\u001b[0m\n",
      "\u001b[1mGPT4O_P1\u001b[0m gpt-4o complex prompt 1 \u001b[94mprompt_template_1\u001b[0m\n",
      "\u001b[1mHAIKU_P1\u001b[0m claude-3-haiku complex prompt 1 \u001b[94mprompt_template_1\u001b[0m\n",
      "\u001b[1mOPUS_P1\u001b[0m claude-3-opus complex prompt 1 \u001b[94mprompt_template_1\u001b[0m\n",
      "\u001b[1mGPT4O_P2\u001b[0m gpt-4o complex prompt 2 \u001b[94mprompt_template_2\u001b[0m\n",
      "\u001b[1mOPUS_P2\u001b[0m claude-3-opus complex prompt 2 \u001b[94mprompt_template_2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"Configurations:\\n\")\n",
    "for k,a in CONFIGURATIONS.items(): \n",
    "    print( TEXT_BOLD + a['id'] + TEXT_END ,a['description'], TEXT_BLUE+ a.get(\"cypher_prompt_template\",\"\") + TEXT_END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a12d4c8b-b538-4ce7-998d-2cab73e968aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mCypher prompt templates:\n",
      "\n",
      "\u001b[94mDEFAULT:\n",
      "\u001b[0m\n",
      "Task:Generate Cypher statement to query a graph database.\n",
      "Instructions:\n",
      "Use only the provided relationship types and properties in the schema.\n",
      "Do not use any other relationship types or properties that are not provided.\n",
      "Schema:\n",
      "{schema}\n",
      "Note: Do not include any explanations or apologies in your responses.\n",
      "Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
      "Do not include any text except the generated Cypher statement.\n",
      "\n",
      "The question is:\n",
      "{question}\n",
      "\n",
      "\n",
      "\u001b[94m\u001b[1mprompt_template_1:\u001b[0m\n",
      "\n",
      "Task:Generate Cypher statement to query a graph database.\n",
      "Instructions:\n",
      "Use only the provided relationship types and properties in the schema.\n",
      "Do not use any other relationship types or properties that are not provided.\n",
      "Always use Cypher CONTAINS expressions when searching on the \"subject\", \"name\" or \"role\" attributes.\n",
      "Always use variables to bind nodes and relationships.\n",
      "Return only the properties that are relevant to the question with descriptive aliases.\n",
      "Use the shortestPath function when asked how two people are connected.\n",
      "Limit the number of iterations to 5 when using variable length quantifiers.\n",
      "Use alias names related to the question.\n",
      "\n",
      "Schema:\n",
      "{schema}\n",
      "\n",
      "Note: Do not include any explanations or apologies in your responses.\n",
      "Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
      "Do not include any text except the generated Cypher statement.\n",
      "\n",
      "Examples: Here are a few examples of generated Cypher statements for particular questions:\n",
      "# Who is John Smith?\n",
      "MATCH (p:Person)-[r:RESPONSIBLE_OF]->(o:Organization)\n",
      "WHERE p.name contains \"John Smith\"\n",
      "RETURN p.name, r.role, o\n",
      "\n",
      "# How is John Smith related with Peter Pan?\n",
      "MATCH p = shortestPath((a)-[*..10]-(b))\n",
      "WHERE a.name contains \"John\" AND b.name contains \"Peter Pan\"\n",
      "RETURN p\n",
      "\n",
      "\n",
      "The question is:\n",
      "{question}\n",
      "\n",
      "\n",
      "\u001b[94m\u001b[1mprompt_template_2:\u001b[0m\n",
      "\n",
      "Task:Generate Cypher statement to query a graph database.\n",
      "Instructions:\n",
      "Use only the provided relationship types and properties in the schema.\n",
      "Do not use any other relationship types or properties that are not provided.\n",
      "Always use Cypher case-insensitive regular expressions when searching on the \"subject\", \"name\" or \"role\" attributes.\n",
      "Always use variables to bind nodes and relationships.\n",
      "Always consider the relationship direction.\n",
      "Return only the properties that are relevant to the question with descriptive aliases.\n",
      "Use the shortestPath function when asked how two people are connected.\n",
      "Limit the number of iterations to 5 when using variable length quantifiers.\n",
      "Use alias names related to the question.\n",
      "Make sure that any attribute used for filtering is also incorporated in the query results.\n",
      "Schema:\n",
      "{schema}\n",
      "\n",
      "Additional information about the graph schema:\n",
      "Graph nodes:\n",
      "- Organization: Refers to the structure of the Government of Catalonia.\n",
      "- Group: Refers to groups of interests who lobbie with the Government. It could be companies, schools, ngo or any other association.\n",
      "- Person: Refers to public servants or individuals representing groups of interests.\n",
      "- Event: Refers to events where Government and Groups of interest meet for different reasons.\n",
      "- Agreement: Refers to formal binding agreements between Government and groups of interests.\n",
      "Graph relationships:\n",
      "- RESPONSIBLE_OF: Indicates who is in charge of a given position (role) on a given period of time.\n",
      "- PARTICIPATE: Indicates who participated in a given event. In the case of the person the \"role\" property tells the role that the person had at that time.\n",
      "- LINKED_TO: Connects the different agreements which are related to a master agreement.\n",
      "\n",
      "Note: Do not include any explanations or apologies in your responses.\n",
      "Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n",
      "Do not include any text except the generated Cypher statement.\n",
      "\n",
      "Examples: Here are a few examples of generated Cypher statements for particular questions:\n",
      "# Who is John Smith?\n",
      "MATCH (p:Person)-[r:RESPONSIBLE_OF]->(o:Organization) \n",
      "WHERE p.name =~ \"(?i).*John Smith.*\" \n",
      "RETURN p.name as Person ,r.role,r.date_from,r.date_to,o.name as Organization\n",
      "\n",
      "# How is John Smith related with Peter Pan?\n",
      "MATCH p = shortestPath((a)-[*..5]-(b))\n",
      "WHERE a.name =~ \"(?i).*John.*\"  AND b.name =~ \"(?i).*Peter Pan.*\" \n",
      "RETURN p\n",
      "\n",
      "# Quines persones han estan més actives en relació a la sequera? Mostra la llista de les 5 persones per any.\n",
      "MATCH (p:Person)-[r:PARTICIPATE]->(e:Event) \n",
      "WHERE e.subject =~ \"(?i).*sequera.*\" \n",
      "RETURN p.name AS Person, e.date.year AS Year, COUNT(e) AS Activity_Count \n",
      "ORDER BY Activity_Count DESC LIMIT 5\n",
      "\n",
      "The question is:\n",
      "{question}\n"
     ]
    }
   ],
   "source": [
    "# Print templates\n",
    "print(TEXT_BOLD + \"Cypher prompt templates:\\n\\n\" + TEXT_BLUE + \"DEFAULT:\\n\" + TEXT_END)\n",
    "print(CYPHER_GENERATION_PROMPT.template)\n",
    "\n",
    "for k,a in TEMPLATES.items(): print(\"\\n\\n\" + TEXT_BLUE + TEXT_BOLD + k +\":\" + TEXT_END +\"\\n\\n\" + a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e154159f-cee2-4ed7-bca5-06ca800f0105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions:\n",
      "\n",
      "\u001b[1mQ001\u001b[0m Qui és el president de la Generalitat de Catalunya?\n",
      "\u001b[1mQ002\u001b[0m Qui té actualment el càrrec de 'President de la Generalitat de Catalunya'?\n",
      "\u001b[1mQ003\u001b[0m Qui és el responsable de Direcció General de Turisme?\n",
      "\u001b[1mQ004\u001b[0m Quina és la estructura del Parlament de Catalunya?\n",
      "\u001b[1mQ005\u001b[0m Quines reunions s'han celebrat amb el Grup Universitat Oberta de Catalunya?\n",
      "\u001b[1mQ006\u001b[0m Qui es va reunir amb el grup Universitat Oberta de Catalunya?\n",
      "\u001b[1mQ007\u001b[0m Qui és Jaume Giró Ribas?\n",
      "\u001b[1mQ008\u001b[0m Quan es va reunir en Tomàs Roy Català amb el grup Universitat Oberta de Catalunya? I quin tema es va tractar?\n",
      "\u001b[1mQ009\u001b[0m Amb qui s'ha reunit en Tomàs Roy Català?\n",
      "\u001b[1mQ010\u001b[0m Llista les 10 persones amb més carrecs\n",
      "\u001b[1mQ011\u001b[0m Parlam sobre Elisenda Guillaumes Cullell?\n",
      "\u001b[1mQ012\u001b[0m Quina és la relació entre 'Miquel Salazar Canalda' i 'Joan Vintró Castells'? Descriu la relació pas a pas.\n",
      "\u001b[1mQ013\u001b[0m Quins grups s'han reunit per tractar sobre la sequera? Incloure el tema de la reunió i la data.\n",
      "\u001b[1mQ014\u001b[0m Amb quins grups s'ha parlat més sobre la sequera?\n",
      "\u001b[1mQ015\u001b[0m Quines persones han estan més actives en relació a la sequera? Mostra la llista de les 5 persones per any.\n",
      "\u001b[1mQ016\u001b[0m Quins organismes han mantingut més reunions per tractar sobre la sequera?\n",
      "\u001b[1mQ017\u001b[0m Quina va ser la reunió amb més participants l'any 2023?\n",
      "\u001b[1mQ018\u001b[0m Què saps del conveni 2022/9/0304?\n",
      "\u001b[1mQ019\u001b[0m Qui ha signat el conveni 2022/9/0304? Inclou tots els convenis relacionats. Llista les entitats i el nom de la persona i el seu càrrec.\n",
      "\u001b[1mQ020\u001b[0m Llista totes les persones, organitzacions i grups que han signat el conveni 2022/9/0304. Inclou tots els convenis relacionats amb el 2022/9/0304.\n"
     ]
    }
   ],
   "source": [
    "print (\"Questions:\\n\")\n",
    "for q in QUESTIONS: print(TEXT_BOLD + q[\"id\"] + TEXT_END, q[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a3962f-5614-4f6c-a63f-3a5728d49783",
   "metadata": {},
   "source": [
    "# Instanciate LangChain Chats LLMs\n",
    "\n",
    "The LLMS dict hosts the LangChain Chats to access the LLMs in the scope of the POCs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "080a8a2a-5fef-4eed-ac64-41dc136a68c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chats to interact with the LLMs.\n",
    "LLMS={}\n",
    "\n",
    "# OpenAI models\n",
    "LLMS[\"gpt-3.5-turbo\"] = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "# LLMS[\"gpt-4-turbo\"]   = ChatOpenAI(temperature=0, model=\"gpt-4-turbo\")\n",
    "LLMS[\"gpt-4o\"]        = ChatOpenAI(temperature=0, model=\"gpt-4o\") # gpt-4o released 2024-05-13\n",
    "\n",
    "# Anthropic models: https://docs.anthropic.com/en/docs/models-overview#model-comparison\n",
    "LLMS[\"claude-3-haiku\"] = ChatAnthropic(temperature=0, model_name=\"claude-3-haiku-20240307\")\n",
    "LLMS[\"claude-3-opus\"]  = ChatAnthropic(temperature=0, model_name=\"claude-3-opus-20240229\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f62ca6c-2d71-44bf-a0ba-da2378946dae",
   "metadata": {},
   "source": [
    "# Procedures to execute the Q&A tests\n",
    "\n",
    "- now(): returns the current timestamp in ISO format.\n",
    "- create_chain(configuration): creates a GraphCypherQAChain object from the given configuration.\n",
    "- ask_questions(configuration, questions, run_id=None, comment=None): invokes the given model to answer a lists of questions. the results are stored in a JSON files for analysis.\n",
    "- print_run(run): prints the run results for proper documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfb92813-3389-44a8-b797-a1ede7af9fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def now():\n",
    "    return datetime.datetime.now(datetime.UTC).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    \n",
    "def create_chain(configuration):\n",
    "    \"\"\"Creates an instance of a GraphCypherQAChain from a given configuration\n",
    "    The configuration indicates the LLM (Chat) to use and the prompt template to\n",
    "    generate the Cypher queries.\n",
    "\n",
    "    In all cases the Department label is excluded to provide the LLM with a simpler graph schema.\n",
    "    This does not exclude any node because Departments are also Organizations.\n",
    "    \n",
    "    Args:\n",
    "        configuration: Dict with the required configuration (from configuration.json file).\n",
    "\n",
    "    Returns:\n",
    "        A GraphCypherQAChain chain \n",
    "    \"\"\"\n",
    "\n",
    "    # Create a prompt for Cypher code generation if there is a template specified in the configuration\n",
    "    cypher_prompt = None\n",
    "    cypher_prompt_template = configuration.get(\"cypher_prompt_template\")\n",
    "    if cypher_prompt_template:\n",
    "        cypher_prompt = PromptTemplate(\n",
    "            input_variables=[\"schema\", \"question\"], template=TEMPLATES[cypher_prompt_template]\n",
    "        )\n",
    "\n",
    "    # Since Departments are also Organizations, we exclude them to provide the LLM with a slightly simpler schema.\n",
    "    exclude_types= [\"Department\"]\n",
    "    \n",
    "    chain = GraphCypherQAChain.from_llm(\n",
    "        llm = LLMS[configuration[\"llm\"]],\n",
    "        cypher_prompt = cypher_prompt,\n",
    "        graph=graph, \n",
    "        verbose=False, \n",
    "        return_intermediate_steps=True,\n",
    "        exclude_types = exclude_types\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "\n",
    "def ask_questions(configuration, questions, run_id=None, comment=None):\n",
    "    \"\"\"Sends the list of questions to the GraphCypherQAChain and stores the results in a JSON file.\n",
    "\n",
    "    The GraphCypherQAChain answers the questions in three steps. \n",
    "        1) The question in Natural Language is converted to a Cypher query.\n",
    "        2) The Cypher query is sent to Neo4j.\n",
    "        3) The results of the Cypher query and the original query are sent to the LLM to get the answer in natural language.\n",
    "\n",
    "    The questions and answers plus execution metadata are stored in the \"poc1_answers/<run_id>.json\" file for further evaluation.\n",
    "    For models with limited throughput a pause is introduced to respect the corresponding request per minute rate.\n",
    "    \n",
    "    If LangSmith is configured (recommended) questions are tagged with \"POC1\" and metadata with \n",
    "    configuration id, llm name and question id is provided.\n",
    "\n",
    "    Args:\n",
    "        configuration: Dict with the configuration to create a GraphCypherQAChain.\n",
    "        questions: List of questions to send to the LLM via the Chain.\n",
    "        run_id: Name of the JSON file where the answers to the questions are stored.\n",
    "        comment: Optional comment added in the \n",
    "\n",
    "    Returns:\n",
    "        A tuple with the Dict with the Questions and Answers and the GraphCypherQAChain\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Run Id:\", run_id)\n",
    "    print(\"Configuration:\",configuration[\"id\"],configuration[\"description\"])\n",
    "    print()\n",
    "    \n",
    "    chain = create_chain(configuration)\n",
    "\n",
    "    sleep_between_questions = 60.0 / configuration.get(\"requests_per_minute\") if configuration.get(\"requests_per_minute\") else 0.0\n",
    "        \n",
    "    results = {\"run_id\": run_id,\"comment\": comment, \"start_time\": now()}\n",
    "    run_start_time = time.time()\n",
    "    \n",
    "    # Tags and metadata information for LangSmith\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = LANGCHAIN_PROJECT\n",
    "    config = {\"tags\":[\"POC1\"],\"metadata\":{\"config\": configuration[\"id\"],\"llm\": configuration[\"llm\"]}}\n",
    "    if run_id: config[\"metadata\"][\"run_id\"] = run_id\n",
    "\n",
    "    answers = []\n",
    "    for idx,q in enumerate(questions):\n",
    "\n",
    "        # Wait for next call to avoid error 429 with Claude\n",
    "        if idx > 0 and (sleep_between_questions - result_metadata[\"query_time\"])>0:\n",
    "            wait_s = sleep_between_questions - result_metadata[\"query_time\"]\n",
    "            print(\" wait(s):\", wait_s)\n",
    "            time.sleep(wait_s)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        query_start_time = now()\n",
    "        result = {\"question\": q}\n",
    "        result_metadata ={}\n",
    "\n",
    "        # Send question to the LLM via the Chain\n",
    "        print(\"\\nQuestion: \", q)\n",
    "\n",
    "        config[\"metadata\"][\"question\"] = q[\"id\"]\n",
    "        \n",
    "        try:\n",
    "            with get_openai_callback() as cb:\n",
    "                result[\"answer\"] = chain.invoke(q[\"question\"],config = config)\n",
    "            if cb.total_tokens > 0:\n",
    "                result_metadata.update({\"prompt_tokens\": cb.prompt_tokens,\n",
    "                                        \"completion_tokens\": cb.completion_tokens,\n",
    "                                        \"total_tokens\": cb.total_tokens,\n",
    "                                        \"total_cost\": cb.total_cost})\n",
    "\n",
    "        except Exception as e:\n",
    "            result[\"answer\"] = {\"query\": q[\"question\"], \"result\": str(e)}\n",
    "        \n",
    "        print(\"Answer: \", result[\"answer\"][\"result\"])\n",
    "        \n",
    "        result_metadata[\"start_time\"] = query_start_time\n",
    "        result_metadata[\"end_time\"] = now()\n",
    "        result_metadata[\"query_time\"] = time.time() - start_time\n",
    "        result[\"metadata\"] = result_metadata\n",
    "        \n",
    "        answers.append(result)\n",
    "\n",
    "    results[\"end_time\"] = now()\n",
    "    results[\"total_time\"] = time.time() - run_start_time\n",
    "    results[\"num_questions\"] = len(answers)\n",
    "    results[\"configuration\"] = configuration\n",
    "    results[\"questions\"] = answers\n",
    "\n",
    "    # Results are saved in a json file for further analysis.\n",
    "    if run_id:\n",
    "        save_jsonpickle(os.path.join(ANSWERS_FOLDER,f\"{run_id}.json\"), results)\n",
    "    \n",
    "    return (results, chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "034bb9de-3a0c-4822-8788-e06fa788a220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_run(run):\n",
    "    \"\"\" Helper procedure to produce a user friendy printed version of a given run (JSON).\n",
    "\n",
    "    Args:\n",
    "        run: Dict with the run (JSON) to print.\n",
    "    \"\"\"\n",
    "        \n",
    "    print(\"Run ID:\", TEXT_BOLD + run.get(\"run_id\") + TEXT_END, run.get(\"comment\"))\n",
    "    print(\"Config: \" + run[\"configuration\"][\"id\"] + \" \" + run[\"configuration\"][\"description\"])\n",
    "    print(\"Time:  \", run.get(\"start_time\"), \" - \", run.get(\"end_time\"), \"\\n\\nQuestions:\")\n",
    "\n",
    "    for q in run[\"questions\"]:\n",
    "        print(\"\\n\"+ TEXT_BOLD + q[\"question\"][\"id\"] + \" \" + TEXT_BLUE + q[\"question\"][\"question\"] + TEXT_END +\"\\n\")\n",
    "        #print(\"Query:\")\n",
    "        print(TEXT_GREEN + q[\"answer\"][\"intermediate_steps\"][0][\"query\"] + TEXT_END)\n",
    "        print(\"\\n\" + q[\"answer\"][\"result\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caea6af-2fd1-4b3c-bb40-ba133a99d082",
   "metadata": {},
   "source": [
    "## Execute the tests\n",
    "\n",
    "### Connect to Graph\n",
    "If using Neo4j Desktop, ensure that the Neo4j Graph Engine is up and running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd492415-6302-4529-9fd5-245fb9a6a28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties:\n",
      "Organization {last_event_date: DATE, code: STRING, id: INTEGER, name: STRING, first_event_date: DATE, pk: STRING}\n",
      "Person {name: STRING, pk: STRING}\n",
      "Department {name: STRING, id: INTEGER, first_event_date: DATE, last_event_date: DATE, pk: STRING, code: STRING}\n",
      "Group {name: STRING, url: STRING, proporals: STRING, type: STRING, id: STRING, has_events: BOOLEAN, pk: STRING, mission: STRING}\n",
      "Event {time: LOCAL_TIME, date: DATE, subject: STRING}\n",
      "Agreement {subject: STRING, signees_gencat: STRING, signees_other: STRING, signature_date: DATE, validity_date: DATE, code: STRING, title: STRING, document: STRING}\n",
      "Relationship properties:\n",
      "RESPONSIBLE_OF {role: STRING, date: DATE, date_from: DATE, date_to: DATE}\n",
      "PARTICIPATE {role: STRING}\n",
      "The relationships:\n",
      "(:Organization)-[:CHILD_OF]->(:Organization)\n",
      "(:Organization)-[:CHILD_OF]->(:Department)\n",
      "(:Organization)-[:PARTICIPATE]->(:Event)\n",
      "(:Person)-[:RESPONSIBLE_OF]->(:Organization)\n",
      "(:Person)-[:RESPONSIBLE_OF]->(:Department)\n",
      "(:Person)-[:PARTICIPATE]->(:Event)\n",
      "(:Department)-[:CHILD_OF]->(:Organization)\n",
      "(:Department)-[:PARTICIPATE]->(:Event)\n",
      "(:Group)-[:PARTICIPATE]->(:Event)\n",
      "(:Agreement)-[:LINKED_TO]->(:Agreement)\n"
     ]
    }
   ],
   "source": [
    "# Connecting to the Neo4j graph (Desktop)\n",
    "graph = Neo4jGraph(url = NEO4J_URL, username = NEO4J_USERNAME, password = os.environ['NEO4J_PWD'])\n",
    "\n",
    "# Print the graph schema\n",
    "print(graph.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bbad08-006d-412a-95fc-77c0c52b1d32",
   "metadata": {},
   "source": [
    "### Execute each test run\n",
    "\n",
    "The *ask_questions* function is used to test the different configurations.\n",
    "The test is executed in two stages:\n",
    "\n",
    "* Stage 1: Using the initial graph created from the CSV files where the **Agreement** nodes are NOT connected to the rest of the graph.\n",
    "\n",
    "\n",
    "Convert the following cell from \"Raw\" to \"Code\" to perform the actual tests.<br>Else the JSON files with the results of each run are available in the \"poc1_answers\" folder.\n",
    "\n",
    "**REMEMBER**: There are some costs associated to the LLMs usage (token consumption) for running the tests. (It should be less than one USD though). Actual token usage (and cost estimate) is available in the json files for OpenAI. Token usage per call is available in LangSmith.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f12ae7d-6937-4812-8e72-08840fb8db0d",
   "metadata": {},
   "source": [
    "# Execution of the test for Stage 1 (before POC2).\n",
    "\n",
    "_ = ask_questions(CONFIGURATIONS[\"GPT35_P0\"],QUESTIONS,\"R01\", \"Complete run\")\n",
    "_ = ask_questions(CONFIGURATIONS[\"GPT4O_P0\"],QUESTIONS,\"R02\", \"Complete run\")\n",
    "_ = ask_questions(CONFIGURATIONS[\"GPT35_P1\"],QUESTIONS,\"R03\", \"Complete run\")\n",
    "_ = ask_questions(CONFIGURATIONS[\"GPT4O_P1\"],QUESTIONS,\"R04\", \"Complete run\")\n",
    "_ = ask_questions(CONFIGURATIONS[\"HAIKU_P1\"],QUESTIONS,\"R05\", \"Complete run\")\n",
    "_ = ask_questions(CONFIGURATIONS[\"OPUS_P1\"], QUESTIONS,\"R06\", \"Complete run\")\n",
    "_ = ask_questions(CONFIGURATIONS[\"GPT4O_P2\"],QUESTIONS,\"R07\", \"Complete run\")\n",
    "_ = ask_questions(CONFIGURATIONS[\"OPUS_P2\"], QUESTIONS,\"R08\", \"Complete run\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d234c3a-c8e4-4049-ad57-e9f97b801929",
   "metadata": {},
   "source": [
    "* Stage 2: Run another set of tests AFTER the completion of the POC2 (Enrich graph). After the POC2 the nodes belonging to the **Agreement** 2022/9/0304 should be connected to \"Person\", \"Organization\" and \"Group\" nodes.\n",
    "\n",
    "In this case we are interested on the last 3 questions of the dataset which especifically refer to that agreement.\n",
    "\n",
    "AFTER running the POC2, the graph schema needs to be reloaded by executing the Connect to Graph cell above.<br>\n",
    "The new :SIGNED and :REPRESENTS relationships should show in the cell output."
   ]
  },
  {
   "cell_type": "raw",
   "id": "edb9e99a-9f58-48f3-b1ad-1d2ca11d4344",
   "metadata": {},
   "source": [
    "_ = ask_questions(CONFIGURATIONS[\"GPT4O_P2\"],QUESTIONS[17:20],\"R09\", \"test Q018:Q020 AFTER POC2 using template 2\")\n",
    "_ = ask_questions(CONFIGURATIONS[\"OPUS_P2\"], QUESTIONS[17:20],\"R10\", \"test Q018:Q020 AFTER POC2 using template 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f96216-e4dd-40d3-b437-3fae7f1a1ce2",
   "metadata": {},
   "source": [
    "**POC 1 Notebook ends here.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
